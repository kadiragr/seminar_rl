{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40d0a41-43ae-4267-92f1-360a1bd9884c",
   "metadata": {},
   "source": [
    "## **Tailoring QR-DQN to Seaquest**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15f872-16f6-477c-aaf2-409ce3d1d861",
   "metadata": {},
   "source": [
    "This report analyzes how QR-DQN (Quantile Regression DQN) can be tailored to the Atari game Seaquest. The goal is to evaluate how well the algorithm performs in this environment and how specific changes to the training setup influence its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f5f0f-2f53-440d-9aa1-5d1b476f37c2",
   "metadata": {},
   "source": [
    "## 1. Seaquest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f523d40-45eb-4da8-b383-b32966126e56",
   "metadata": {},
   "source": [
    "### 1.1 Seaquest (Atari 2600)\n",
    "\n",
    "Seaquest is an Atari 2600 game in which a player controls a submarine. The main goal is to rescue divers and destroy enemies. The player must manage limited oxygen, which makes regular surfacing important.\n",
    "\n",
    "The submarine can move in all directions and shoot torpedoes. The player has a limited number of lives. Oxygen continuously decreases while the submarine is underwater and is shown by an on-screen indicator. To refill oxygen, the player must surface.\n",
    "\n",
    "If the oxygen level reaches zero, the player loses one life. When surfacing with at least one rescued diver, exactly one diver is safely delivered. Surfacing without carrying a diver results in the loss of one life. This creates a risk–reward trade-off between staying underwater longer and surfacing in time.\n",
    "\n",
    "The scoring system rewards both rescuing divers and destroying enemies. Destroying an enemy grants 20 points. After rescuing six divers, each diver is worth 50 points, and an additional oxygen bonus is awarded. After every set of six rescued divers, the game difficulty increases: the points per diver increase by 50, up to a maximum of 1000 points, and the points for destroying enemies increase by 10, up to a maximum of 90 points.\n",
    "\n",
    "There are two main enemy types. Sharks actively follow the divers, and colliding with them causes the player to lose one life. Enemy submarines move horizontally, often near the surface, and fire torpedoes in their direction of movement. Being hit by a torpedo or colliding with an enemy submarine also results in the loss of one life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141b31e-93b7-478d-ba4d-e203ed9b3409",
   "metadata": {},
   "source": [
    "### 1.2 Seaquest Environment (Gymnasium)\n",
    "\n",
    "The Seaquest environment is part of the Atari environments provided by Gymnasium. In this report, the version  \n",
    "`gymnasium.make(\"ALE/Seaquest-v4\")` is used.\n",
    "\n",
    "![Seaquest gameplay](seaques.gif)\n",
    "\n",
    "#### Observation Space\n",
    "\n",
    "By default, Seaquest uses an RGB image as observation.\n",
    "\n",
    "- **Observation Space:** `Box(0, 255, (210, 160, 3), uint8)`\n",
    "\n",
    "This represents the raw game screen with a resolution of 210 × 160 pixels and three color channels.  \n",
    "Alternative observation types such as RAM `(128,)` and grayscale `(210, 160)` are also available and can be used for comparison.\n",
    "\n",
    "#### Action Space\n",
    "\n",
    "Seaquest uses a discrete action space:\n",
    "\n",
    "- **Action Space:** `Discrete(18)`\n",
    "\n",
    "Each action is a movement direction, firing torpedoes, or a combination of both.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8246073-e4e8-4be5-859b-905cb9d3a61b",
   "metadata": {},
   "source": [
    "#### Table 1: Discrete Action Space of Seaquest\n",
    "\n",
    "| Action ID | Meaning              |\n",
    "|----------:|----------------------|\n",
    "| 0         | NOOP                 |\n",
    "| 1         | FIRE                 |\n",
    "| 2         | UP                   |\n",
    "| 3         | RIGHT                |\n",
    "| 4         | LEFT                 |\n",
    "| 5         | DOWN                 |\n",
    "| 6         | UPRIGHT              |\n",
    "| 7         | UPLEFT               |\n",
    "| 8         | DOWNRIGHT            |\n",
    "| 9         | DOWNLEFT             |\n",
    "| 10        | UP + FIRE            |\n",
    "| 11        | RIGHT + FIRE         |\n",
    "| 12        | LEFT + FIRE          |\n",
    "| 13        | DOWN + FIRE          |\n",
    "| 14        | UPRIGHT + FIRE       |\n",
    "| 15        | UPLEFT + FIRE        |\n",
    "| 16        | DOWNRIGHT + FIRE     |\n",
    "| 17        | DOWNLEFT + FIRE      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780856ca-59be-40be-8b78-45733c61621f",
   "metadata": {},
   "source": [
    "#### Reward System\n",
    "\n",
    "The reward signal is directly based on the in game score. The standard Seaquest reward system is used without modification.  \n",
    "Points are awarded for destroying enemies, rescuing divers, and surfacing with remaining oxygen, as described in Section 1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6624f-adde-4df4-9520-af0921fe971f",
   "metadata": {},
   "source": [
    "## 2. Quantile Regression Deep Q-Network (QR-DQN)\n",
    "\n",
    "In this section, the learning method used in this project is explained. First, the basic idea of Deep Q-Learning is introduced. After that, distributional reinforcement learning and finally QR-DQN are described.\n",
    "\n",
    "### 2.1 Deep Q-Learning\n",
    "\n",
    "Q-Learning is a value-based reinforcement learning method. It learns a function  \n",
    "$Q(s, a)$, which tells how good it is to take action $a$ in state $s$.  \n",
    "The goal is to choose actions that lead to high future rewards.\n",
    "\n",
    "The optimal Q-function follows the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q^*(s,a) = \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s', a') \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "In Deep Q-Networks (DQN), the Q-function is approximated with a neural network.  \n",
    "This is needed because for environments like Atari games the state space is too large to store all Q-values in a table.\n",
    "\n",
    "The network is trained by minimizing the difference between predicted Q-values and target values:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}\\left[\\left( y - Q_\\theta(s,a) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "To make training more stable, techniques like replay buffers, target networks and exploration strategies are used.\n",
    "\n",
    "### 2.2 Distributional Reinforcement Learning\n",
    "\n",
    "In standard DQN, the Q-value represents only the expected return.  \n",
    "However, future rewards are random and can vary a lot.\n",
    "\n",
    "The return can be written as:\n",
    "\n",
    "$$\n",
    "Z(s,a) = \\sum_{t=0}^{\\infty} \\gamma^t r_t\n",
    "$$\n",
    "\n",
    "Distributional reinforcement learning tries to learn the full return distribution $Z(s,a)$ instead of only its expected value:\n",
    "\n",
    "$$\n",
    "Q(s,a) = \\mathbb{E}[Z(s,a)]\n",
    "$$\n",
    "\n",
    "This gives the agent more information about possible outcomes and uncertainty.  \n",
    "In practice, this often makes learning more stable.\n",
    "\n",
    "### 2.3 Quantile Regression DQN (QR-DQN)\n",
    "\n",
    "QR-DQN is an extension of DQN that models the return distribution using quantiles.  \n",
    "Instead of predicting one Q-value per action, the network predicts multiple values.\n",
    "\n",
    "For each state-action pair, the network outputs $N$ quantiles:\n",
    "\n",
    "$$\n",
    "Z_\\theta(s,a) = \\{ \\theta_1(s,a), \\dots, \\theta_N(s,a) \\}\n",
    "$$\n",
    "\n",
    "Each quantile corresponds to a probability level:\n",
    "\n",
    "$$\n",
    "\\tau_i = \\frac{i - 0.5}{N}, \\quad i = 1, \\dots, N\n",
    "$$\n",
    "\n",
    "Training is done using quantile regression with a special loss function.  \n",
    "Compared to normal DQN, QR-DQN has the same general training setup, but learns bit more stable and can achieve better performance, especially in more complex environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628d383-a11c-4baf-a1cf-d4e1af4bb3f5",
   "metadata": {},
   "source": [
    "## 3. Baseline: Hyperparameter & Training\n",
    "\n",
    "This section describes the baseline setup used for training QR-DQN on Seaquest.  \n",
    "\n",
    "### 3.1 Baseline Hyperparameters (rl-zoo Atari)\n",
    "\n",
    "The baseline hyperparameters are taken from the Atari configurations provided by rl-zoo.  \n",
    "Compared to many other reinforcement learning algorithms, relatively few hyperparameters are explicitly specified in this setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be110c71-4df9-4a14-89f1-e6c73c7a315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atari:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  policy: 'CnnPolicy'\n",
    "  n_timesteps: !!float 1e7\n",
    "  learning_starts: 50000\n",
    "  exploration_fraction: 0.025  # explore 250k steps = 10M * 0.025\n",
    "  # If True, you need to deactivate handle_timeout_termination\n",
    "  # in the replay_buffer_kwargs\n",
    "  optimize_memory_usage: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326f851-4eaf-475f-bb4c-3efebe1f9e68",
   "metadata": {},
   "source": [
    "Most remaining hyperparameters are assumed to be already well chosen for Atari environments. Thats why, the focus is less on fine-tuning hyperparameters. Only small adjustments are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a036cc9-ce19-4765-9b67-55850c06e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SeaquestNoFrameskip-v4:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  policy: 'CnnPolicy'\n",
    "  n_timesteps: 8400000\n",
    "  learning_starts: 50000\n",
    "  exploration_fraction: 0.05  # explore 500k steps = 10M * 0.05\n",
    "  buffer_size: 200000\n",
    "  optimize_memory_usage: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f50a3-ff3b-460d-84bf-65b6bd07d094",
   "metadata": {},
   "source": [
    "First, the replay buffer size was reduced from 1,000,000 to 200,000.  \n",
    "The default value of 1M seemed too large, and 200k felt like a better compromise than going with 100k, like other algorithms.\n",
    "\n",
    "Second, the exploration fraction was increased from 0.025 to 0.05.  \n",
    "This lets the agent explore for a longer time and helps to avoid converging too early to bad strategies.\n",
    "\n",
    "Third, the total training time was reduced from 10 million to 8.4 million timesteps.  \n",
    "After around 8 million steps, only little improvement was observed, so it was set to 8.4 million to get all the training done faster.\n",
    "\n",
    "\n",
    "The standard SB3 AtariWrapper is used. This wrapper applies common preprocessing steps such as:\n",
    "- frame skipping and frame stacking,\n",
    "- conversion to grayscale images,\n",
    "- resizing of observations,\n",
    "- reward clipping to the range [-1, 1].\n",
    "\n",
    "These steps simplify the input space and help to stabilize training.\n",
    "\n",
    "### 3.3 Training and Results\n",
    "The final baseline runs were done with 8.4 million timesteps and with a total of 20 seeds.\n",
    "\n",
    "The training curves show a consistent improvement in performance over the timesteps, but also a increasing variance between seeds. While the average score increases steadily, the individual runs start to diverge more strongly.\n",
    "\n",
    "![baseline_eval](Picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63f4f1-6934-4e4d-967d-c24c06c5999b",
   "metadata": {},
   "source": [
    "This behavior becomes clearer when looking at the individual training curves. In many runs, the agent suddenly discovers a new strategy, leading to a rapid increase in score. However, the point at which this happens differs strongly between seeds. In some cases, this improvement happens early, while in others it occurs much later or not at all.\n",
    "\n",
    "![baseline_indi](Picture2.png)\n",
    "\n",
    "For example, some seeds never manage to move beyond a score of 1000 points, even after 8.4 million timesteps, while other seeds reach 4000 points or more. This explains the growing variance observed in the aggregated training curves.\n",
    "\n",
    "Overall, the training curves suggest that learning in Seaquest happens in distinct stages. The agent often stabilizes around certain performance levels, such as around 900 points, then later around 2000 points, and finally around 4000 points or higher. \n",
    "\n",
    "These observations suggest that the results can be used to guide further improvements.  \n",
    "Since good strategies appear suddenly and at very different times, the goal is to speed up learning, so that better strategies are discovered earlier and more consistently across seeds.\n",
    "\n",
    "Looking at individual agents will give us a better insight.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Picture6.png\" width=\"360\"></td>\n",
    "    <td><img src=\"Picture7.png\" width=\"360\"></td>\n",
    "    <td><img src=\"Picture8.png\" width=\"360\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Picture3.gif\" width=\"360\"></td>\n",
    "    <td><img src=\"Picture4.gif\" width=\"360\"></td>\n",
    "    <td><img src=\"Picture5.gif\" width=\"360\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The best-performing agent (seed 4) reaches a mean reward of over 5000 points.  \n",
    "When observing its gameplay, it can be seen that this agent consistently collects divers and only surfaces to refill oxygen when it is carrying at least one diver. It also avoids enemies well and survives for long periods of time.\n",
    "\n",
    "At the same time, this agent behaves quite conservatively. It mostly stays in the upper half of the screen and avoids risky dives to deeper divers. While this strategy is safe and stable, it also limits the maximum score that can be achieved.\n",
    "\n",
    "The medium-performing agent shows a similar overall strategy, but executes it less effectively.  \n",
    "It stays even closer to the surface, collects fewer divers, and therefore reaches lower scores. The basic idea of the strategy is present, but it is applied more cautiously and with worse timing.\n",
    "\n",
    "The worst-performing agent behaves very differently.  \n",
    "It repeatedly dives down one level, shoots continuously in one direction, and waits for enemies to spawn. After destroying them, it returns to the same position and repeats this behavior. Over time, it runs out of oxygen and dies. This agent has not learned that collecting divers and surfacing for oxygen are the most important features in this game.\n",
    "\n",
    "Overall, these examples show that performance differences mainly depend on how well the agent understands diver collection, oxygen management, and risk. Agents that learn these concepts early perform much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe66f1f-99f6-4833-96ca-6e8d4053901c",
   "metadata": {},
   "source": [
    "## 4. Action-Space Adjustment\n",
    "\n",
    "This section describes an experiment where the action space of Seaquest was reduced to simplify learning.\n",
    "\n",
    "### 4.1 Motivation\n",
    "\n",
    "Compared to other Atari games, Seaquest has a larger action space with 18 actions.  \n",
    "Many of these actions are combinations of movement and shooting, this creates redundancy and increases the complexity of exploration.\n",
    "\n",
    "The idea was that a large action space makes it harder for the agent to explore efficiently, especially in the early phase of training. Therefore, reducing the action space could help the agent to learn faster.\n",
    "\n",
    "### 4.2 Analysis of Action Usage\n",
    "\n",
    "To decide which actions are important, the action usage of the best baseline agent was analyzed for a single episode.\n",
    "\n",
    "<h4>Table 2: Action Usage</h4>\n",
    "\n",
    "<table style=\"margin-left: 0;\">\n",
    "  <tr>\n",
    "    <th align=\"left\">Action ID</th>\n",
    "    <th align=\"left\">Meaning</th>\n",
    "    <th align=\"left\">Usage (%)</th>\n",
    "  </tr>\n",
    "  <tr><td>0</td><td>NOOP</td><td>8.5%</td></tr>\n",
    "  <tr><td>1</td><td>FIRE</td><td>0.4%</td></tr>\n",
    "  <tr><td>2</td><td>UP</td><td>13.5%</td></tr>\n",
    "  <tr><td>3</td><td>RIGHT</td><td>16.1%</td></tr>\n",
    "  <tr><td>4</td><td>LEFT</td><td>11.3%</td></tr>\n",
    "  <tr><td>5</td><td>DOWN</td><td>10.4%</td></tr>\n",
    "  <tr><td>6</td><td>UPRIGHT</td><td>0.8%</td></tr>\n",
    "  <tr><td>7</td><td>UPLEFT</td><td>0.4%</td></tr>\n",
    "  <tr><td>8</td><td>DOWNRIGHT</td><td>1.8%</td></tr>\n",
    "  <tr><td>9</td><td>DOWNLEFT</td><td>1.5%</td></tr>\n",
    "  <tr><td>10</td><td>UP + FIRE</td><td>0.8%</td></tr>\n",
    "  <tr><td>11</td><td>RIGHT + FIRE</td><td>0.4%</td></tr>\n",
    "  <tr><td>12</td><td>LEFT + FIRE</td><td>1.1%</td></tr>\n",
    "  <tr><td>13</td><td>DOWN + FIRE</td><td>1.8%</td></tr>\n",
    "  <tr><td>14</td><td>UPRIGHT + FIRE</td><td>6.7%</td></tr>\n",
    "  <tr><td>15</td><td>UPLEFT + FIRE</td><td>9.3%</td></tr>\n",
    "  <tr><td>16</td><td>DOWNRIGHT + FIRE</td><td>5.5%</td></tr>\n",
    "  <tr><td>17</td><td>DOWNLEFT + FIRE</td><td>9.6%</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "From this, it can be seen that combined movement + fire actions are used to shoot and move.  \n",
    "Pure diagonal movement actions without firing are used only rarely.\n",
    "\n",
    "\n",
    "### 4.3 Adjusted Action Space\n",
    "\n",
    "Based on this observation, the decision was made to remove the pure movement actions and keep:\n",
    "\n",
    "The idea is that combined actions already allow movement and shooting at the same time, which makes separate movement actions less necessary.\n",
    "\n",
    "This reduces the action space from 18 actions to 10 actions, making exploration easier and more focused.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf4af3-ae69-4221-b889-88b97a373b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeaquestOnlyMoveWithFire(_ActionMapWrapper):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        action_map = [\n",
    "            0,   # NOOP\n",
    "            1,   # FIRE\n",
    "            10,  # UPFIRE\n",
    "            11,  # RIGHTFIRE\n",
    "            12,  # LEFTFIRE\n",
    "            13,  # DOWNFIRE\n",
    "            14,  # UPRIGHTFIRE\n",
    "            15,  # UPLEFTFIRE\n",
    "            16,  # DOWNRIGHTFIRE\n",
    "            17,  # DOWNLEFTFIRE\n",
    "        ]\n",
    "        super().__init__(env, action_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b6e3e-469c-46ba-80c5-ecff0075e86c",
   "metadata": {},
   "source": [
    "### 4.4 Training and Observations\n",
    "\n",
    "After reducing the action space, the agent learns faster.  \n",
    "The training curve rises more quickly compared to the baseline. One possible reason is that the agent has fewer actions to explore and can focus on more useful behaviors earlier.\n",
    "\n",
    "Overall this adjustment was a success.\n",
    "\n",
    "![Training curves: baseline vs reduced action space](action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5fb56-8099-425d-adc7-283cd36ad98a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Reward Shaping\n",
    "\n",
    "This section describes an  experiment where the reward function of Seaquest was extended to give the agent more direct feedback.\n",
    "\n",
    "### 5.1 Motivation\n",
    "\n",
    "In the original Seaquest game, rewards are relatively sparse.  \n",
    "Collecting divers is clearly a good action, but it is not rewarded directly.  \n",
    "The agent only receives points later, when surfacing and delivering the divers.\n",
    "\n",
    "This often leads to early deaths and inefficient behavior, especially during the early training phase.  \n",
    "Without direct feedback, learning progress is slow.\n",
    "\n",
    "### 5.2 Modified Reward System\n",
    "\n",
    "To give the agent more guidance, additional rewards were added on top of the original game rewards.\n",
    "\n",
    "To detect when a diver is collected, the RAM representation of the environment was analyzed.  \n",
    "While playing the game manually, all RAM values were logged and compared to in-game events.  \n",
    "Based on this the following RAM indices were identified:\n",
    "\n",
    "- **RAM[62]**: number of currently collected divers  \n",
    "- **RAM[102]**: oxygen level (63 → 0)  \n",
    "- **RAM[97]**: vertical (Y) position of the player\n",
    "\n",
    "Using this information, the reward system was extended as follows:\n",
    "\n",
    "- **Diver collection:** \n",
    "  Every time RAM[62] increases, the agent receives +1.5 reward as direct feedback.\n",
    "\n",
    "- **Surfacing with empty oxygen:**  \n",
    "  If points are scored while oxygen is empty (RAM[102] = 0), the agent receives an additional +1.0 reward.\n",
    "\n",
    "- **Low oxygen behavior:**  \n",
    "  If RAM[102] ≤ 6, every 4 steps it is checked whether the agent moves upward.  \n",
    "  - No upward movement: −0.25 reward  \n",
    "  - Reaching the surface: +1.0 reward\n",
    "\n",
    "### 5.3 Training and Observations\n",
    "\n",
    "With reward shaping, the agent shows a much faster learning progress, especially early in training.  \n",
    "The training curve rises way faster compared to the baseline.\n",
    "\n",
    "This happens because the agent now receives direkt feedback that collecting divers is a good event, instead of learning this only indirectly after surfacing.\n",
    "\n",
    "However, towards the end of training the difference to the baseline becomes smaller.  \n",
    "This can be explained by the fact that many baseline agents eventually also learn that collecting divers and surfacing for oxygen is the correct strategy.\n",
    "\n",
    "In other words, it helps the agent learn earlier, but does not fundamentally change the optimal behavior that is discovered later anyway.\n",
    "\n",
    "Overall, reward shaping improves training speed and stability.\n",
    "\n",
    "![Training curves: reward](reward1.png)\n",
    "\n",
    "![Training curves: reward2](reward2.png)\n",
    "\n",
    "\n",
    "When comparing the best agent trained with reward shaping to the best baseline agent, it can be seen that both follow a very similar overall strategy.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; gap: 40px; align-items: flex-start; margin-top: 20px;\">\n",
    "\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"rewardbestagent.gif\" width=\"420\"><br>\n",
    "    <b>Best Agent (Reward Shaping)</b>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"Picture3.gif\" width=\"420\"><br>\n",
    "    <b>Best Agent (Baseline)</b>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Both agents mainly operate in the upper half of the screen. They shoot enemies, avoid collisions, and try to survive as long as possible. Divers are collected, but only when they are located in the upper region of the map. In addition, both agents usually surface for oxygen only when they are carrying at least one diver and the oxygen level becomes low.\n",
    "\n",
    "Despite this similarity, the reward shaping agent executes this strategy much better. It collects divers more reliably, manages oxygen better, and reaches significantly higher scores. This suggests that reward shaping does not introduce a fundamentally new strategy, but helps the agent to learn and apply an existing good strategy more consistently.\n",
    "\n",
    "Interestingly, both agents also show the same unintended behavior when oxygen is almost empty and no diver is being carried. In this situation they exploit a game mechanic by staying one or two pixels below the surface. At this position, the oxygen level does not decrease.\n",
    "The agents then move slightly up and down while shooting enemies, which allows them to survive for a longer time without actually surfacing. This behavior is a form of bug exploitation rather than intended. But still, it is interesting to observe that both agents independently discover this behavior during training.\n",
    "\n",
    "Overall, the comparison shows that reward shaping, in this case improves how well and how early a good strategy is learned, but not necessarily change the late strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0957eea-f0f4-41df-851e-481061b0f283",
   "metadata": {},
   "source": [
    "## 6. Observation Space Adjustment\n",
    "\n",
    "This section describes an experiment where the observation space was modified to reduce noise and to explicitly provide important game information.\n",
    "\n",
    "### 6.1 Motivation\n",
    "\n",
    "When looking at the Seaquest game screen, it becomes clear that not all visual information is equally relevant for the agent. \n",
    "\n",
    "![SeaquestUI](Picture11.png)\n",
    "\n",
    "\n",
    "Large parts of the UI, such as score display and background are not relevant.\n",
    "\n",
    "The idea was that by removing irrelevant regions of the screen, the agent would receive a cleaner input signal and might learn faster due to reduced noise.\n",
    "\n",
    "![SeaquestUI](Picture12.png)\n",
    "\n",
    "In addition, information that is important for gameplay, could be given to the agent through other channels.  \n",
    "The goal was to explicitly provide information to the agent without changing the CNN policy.\n",
    "\n",
    "To achieve this, two additional channels were added to the observation:\n",
    "- one channel filled with the current value of RAM[102] (oxygen level),\n",
    "- one channel filled with the current value of RAM[62] (number of collected divers).\n",
    "\n",
    "This results in an observation of size 84 × 84 × 3, where:\n",
    "- channel 1 contains the grayscale game screen,\n",
    "- channel 2 encodes the oxygen information,\n",
    "- channel 3 encodes the diver information.\n",
    "\n",
    "Each of these values is constant across the whole channel, which allows the CNN to process them in the same way as image data.\n",
    "\n",
    "![SeaquestUI](Picture13.png)\n",
    "![SeaquestUI](Picture14.png)\n",
    "![SeaquestUI](Picture15.png)\n",
    "\n",
    "### 6.4 Results and Observations\n",
    "\n",
    "![Oberservation Space change Eval](observ1.png)\n",
    "\n",
    "Overall, the results of this experiment were not very strong.  \n",
    "Only a really small improvement was observed at the beginng of training. Towards the later timesteps, performance was similar or even slightly worse than the baseline.\n",
    "\n",
    "In addition, training took significantly longer, due to the increased input size and additional computation.\n",
    "\n",
    "One possible explanation is that the baseline agent already learns to filter out visual noise early during training.  \n",
    "Therefore, explicitly removing parts of the observation does not provide a large benefit.\n",
    "\n",
    "Another reason could be that encoding RAM values as constant image channels is not an efficient way to represent this information. The CNN may not be well suited to interpret such global, non-spatial signals.\n",
    "\n",
    "Overall, chanigng the observation space increased complexity without impoving the performance.\n",
    "\n",
    "![Oberservation Space change Eval](observ2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381bc733-7c19-4429-8159-a43663580040",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusion\n",
    "\n",
    "In conclusion, QR-DQN was adapted to the Atari game Seaquest using several different approaches.  \n",
    "Overall, most changes compared to the baseline improved performance in some way.\n",
    "\n",
    "Reward shaping mainly helped the agent to learn faster. Important behaviors, such as collecting divers and managing oxygen, were discovered much earlier. However, the final performance was often similar to the baseline.\n",
    "\n",
    "The action-space adjustment was the most effective change. Reducing the number of actions clearly improved early learning and led to better and more stable results.\n",
    "\n",
    "The observation-space adjustment did not have a strong impact. Removing visual noise and adding RAM information resulted only in small improvements and sometimes even worse performance. This suggests that the baseline setup already works quite well.\n",
    "\n",
    "Overall, guiding the agent with better rewards and a simpler action space was more useful than changing the observation space.\n",
    "\n",
    "## 8. Future Work\n",
    "\n",
    "Reward shaping could be improved more, for example by increasing rewards per collected diver or by preventing bug exploitation.\n",
    "\n",
    "Further improvements could also come from dedicated hyperparameter tuning or from training directly on the RAM observation space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9050b89-bdd5-4016-aab3-e5eb5a618f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digidata",
   "language": "python",
   "name": "digidata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
